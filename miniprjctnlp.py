# -*- coding: utf-8 -*-
"""miniprjctnlp.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14zy5nv2CfAVF9ZUEYiQFyONkSJZ4wywe
"""

# Import libraries
import pandas as pd

# Load multiple CSV files and combine them
file_paths = ["/tweets-extra.csv", "/tweets-train.csv", "/tweets-valid.csv"]  # Add all your CSV paths here
dataframes = [pd.read_csv(file, encoding="utf-8") for file in file_paths]

# Concatenate all dataframes
df = pd.concat(dataframes, ignore_index=True)

# Display the first 5 rows to verify the data
df.head()

#step 2:EDA
import matplotlib.pyplot as plt
import seaborn as sns

# Check dataset info
print(df.info())
print(df.describe())

# Set smaller figure size (e.g., width=5, height=4)
plt.figure(figsize=(3, 4))  # Adjust dimensions as needed
sns.countplot(x='label', data=df)  # Update 'label' with your target column name
plt.title("Class Distribution")
plt.xlabel("Class")
plt.ylabel("Count")
plt.tight_layout()  # Prevents label cutoff
plt.show()

# Check for missing values
print(df.isnull().sum())

#STEP 3 : PREPROCESSING
import re
import nltk
import pandas as pd
nltk.download('punkt')

# Define custom Marathi stopwords
marathi_stopwords = set([
    "आहे", "नाही", "आणि", "हे", "तुम्ही", "मी", "तो", "ती", "त्याचे", "त्याची", "आम्ही",
    "किंवा", "मध्ये", "यामुळे", "म्हणून", "कसे", "ते", "जे", "तर", "हा", "हि", "या", "याने",
    "पण", "त्याला", "त्यामुळे", "त्यांनी", "कोण", "का", "कधी", "कुठे"
])

# Text cleaning function
def clean_text(text):
    if pd.isnull(text):
        return "MISSING_TEXT"

    text = str(text).lower()  # Lowercase
    text = re.sub(r"http\S+|www\S+|https\S+", "", text)  # Remove URLs
    text = re.sub(r"[!\"#$%&'()*+,./:;<=>?@[\]^_`{|}~]", "", text)  # Remove punctuation
    text = re.sub(r"\d+", "", text)  # Remove numbers
    text = " ".join([word for word in text.split() if word not in marathi_stopwords])  # Remove stopwords
    return text

# Apply text cleaning
# The DataFrame might have a column named 'tweet' instead of 'text'
# Change 'text' to 'tweet' to access the correct column
df["cleaned_text"] = df["tweet"].apply(clean_text)  # Update 'text' to your column name

# Display cleaned data
df[['tweet', 'cleaned_text']].head() # Also change 'text' to 'tweet' here

import numpy as np

# Generate NLP features
def generate_nlp_features(df):
    # Character count
    df['char_count'] = df['cleaned_text'].apply(len)
    # Word count
    df['word_count'] = df['cleaned_text'].apply(lambda x: len(x.split()))
    # Average word length
    df['avg_word_length'] = df['cleaned_text'].apply(lambda x: np.mean([len(word) for word in x.split()]) if x.split() else 0)
    # Stopword count
    df['stopword_count'] = df['cleaned_text'].apply(lambda x: len([word for word in x.split() if word in stop_words]))
    # Hashtag count (if applicable)
    df['htag_count'] = df['cleaned_text'].apply(lambda x: x.count('#'))

    return df

# Generate features
df = generate_nlp_features(df)
df[['char_count', 'word_count', 'avg_word_length', 'stopword_count', 'htag_count']].head()

from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer

# Bag of Words (BoW)
bow_vectorizer = CountVectorizer(max_features=5000)
X_bow = bow_vectorizer.fit_transform(df['cleaned_text'])

# TF-IDF
tfidf_vectorizer = TfidfVectorizer(max_features=5000)
X_tfidf = tfidf_vectorizer.fit_transform(df['cleaned_text'])

# Combine NLP features with TF-IDF for combined feature set
from scipy.sparse import hstack
X_combined = hstack([X_tfidf, df[['char_count', 'word_count', 'avg_word_length', 'stopword_count', 'htag_count']].values])

